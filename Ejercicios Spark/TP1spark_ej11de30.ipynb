{"cells":[{"cell_type":"markdown","metadata":{"id":"KHdU05jEP1EY"},"source":["# Instalo e importo librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnV5re322Wz9","outputId":"fe4d3e4a-b4df-4ee6-f1c3-ab1166e34e47","executionInfo":{"status":"ok","timestamp":1696865614099,"user_tz":180,"elapsed":82368,"user":{"displayName":"FACUNDO NAHUEL FERNANDEZ","userId":"09349471394871366588"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=e7650a9ab286c879f79c8235d7f2faaa53bbd00dbfa169af63a7ffc5cc70809d\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n","The following additional packages will be installed:\n","  libxtst6 openjdk-8-jre-headless\n","Suggested packages:\n","  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum\n","  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n","  fonts-wqy-zenhei fonts-indic\n","The following NEW packages will be installed:\n","  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n","0 upgraded, 3 newly installed, 0 to remove and 18 not upgraded.\n","Need to get 39.7 MB of archives.\n","After this operation, 144 MB of additional disk space will be used.\n","Selecting previously unselected package libxtst6:amd64.\n","(Reading database ... 120875 files and directories currently installed.)\n","Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package openjdk-8-jre-headless:amd64.\n","Preparing to unpack .../openjdk-8-jre-headless_8u382-ga-1~22.04.1_amd64.deb ...\n","Unpacking openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n","Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n","Preparing to unpack .../openjdk-8-jdk-headless_8u382-ga-1~22.04.1_amd64.deb ...\n","Unpacking openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up openjdk-8-jre-headless:amd64 (8u382-ga-1~22.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n","Setting up openjdk-8-jdk-headless:amd64 (8u382-ga-1~22.04.1) ...\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n","update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n","Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n"]}],"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhYIAjti3iaf","outputId":"a553df52-6c78-4793-8fca-13bd39edfdb9","executionInfo":{"status":"ok","timestamp":1696865617027,"user_tz":180,"elapsed":2938,"user":{"displayName":"FACUNDO NAHUEL FERNANDEZ","userId":"09349471394871366588"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext\n","from pyspark.sql import SQLContext\n","import pandas as pd\n","from nltk import word_tokenize\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n"]},{"cell_type":"markdown","metadata":{"id":"oWiDymDPgLil"},"source":["# Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmVhF9Pi3mbH"},"outputs":[],"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"]},{"cell_type":"markdown","metadata":{"id":"iMoY903IgXQZ"},"source":["# Creo sesión de spark y leo csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoqqUqNV3wFZ"},"outputs":[],"source":["downloaded = drive.CreateFile({'id':\"10xgOf2rORcGlcKPME3cHGHssXGzNemer\"})\n","downloaded.GetContentFile('GooglePlayStore_User_Reviews.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ENPglW_4Cco"},"outputs":[],"source":["spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hPSID6HBgE7H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1696865652571,"user_tz":180,"elapsed":11318,"user":{"displayName":"FACUNDO NAHUEL FERNANDEZ","userId":"09349471394871366588"}},"outputId":"ddefdc94-d871-46fe-ebcb-f337a170a61b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}],"source":["sqlContext = SQLContext(sc)\n","df = sqlContext.read.option(\"delimiter\", \",\").option(\"escape\", '\"').csv('GooglePlayStore_User_Reviews.csv', header=True, inferSchema=True)\n","user_reviews = df.rdd"]},{"cell_type":"markdown","metadata":{"id":"n09UTCw4CGjQ"},"source":["# Ejercicio 11\n","\n","Realizar un análisis de stopwords de las reviews. Dada la frecuencia de los tokens de las reviews, mostrar los 30 tokens más frecuentes y listar del total de tokens cuales son stopwords utilizando nltk. (⭐⭐)\n","\n","**ACLARACION:** En slack dijeron que hay que listar las stopwords de los 30 tokens más frecuentes y no del TOTAL, como dice en el enunciado. Se entregó también la otra versión (listando las stopwords del total de tokens) en otro notebook."]},{"cell_type":"markdown","metadata":{"id":"FIUv5H4vUaCh"},"source":["Filtro las reviews nan ya que las considero un error del archivo (y además no me sirven para analizar las palabras de cada una).\n","\n","Uso distinct para eliminar reviews que se encuentran repetidas (provoca que se cuenten más de una vez palabras o tokens).\n","\n","Mapeo para quedarme solo con las reviews.\n","\n","Uso flatMap para quedarme con todos los tokens de cada review (habrá repeticiones por usar flatMap). Como tokenizer utilizo el de nltk.\n","\n","Mapeo para quedarme los tokens (todos en minúscula) como clave y 1 como valor, para desppués sumarlos con reduceByKey, quedándome así con todos los tokens y sus frecuencias.\n","\n","Cacheo porque haré más de una acción sobre esto (tomo los 30 más frecuentes ordenándolos y luego filtro por stopwords y por estos 30 tokens).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHeT24fkTer4"},"outputs":[],"source":["tokens_by_freq = user_reviews.filter(lambda x: (x.Translated_Review != 'nan') & (x.Translated_Review is not None)).distinct()\\\n",".map(lambda x: x.Translated_Review).flatMap(lambda x: word_tokenize(x))\\\n",".map(lambda x: (x.lower(), 1)).reduceByKey(lambda x,y: x+y).cache()"]},{"cell_type":"markdown","metadata":{"id":"gw-9_NNzyUaS"},"source":["Obtengo los tokens más frecuentes ordenando por sus respectivas frecuencias. Podría haber hecho simplemente un takeOrdered( 30, -x[1] ), pero de esta manera obtenía los tokens con sus frecuencias y no me va a servir para lo que quiero hacer más adelante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7Le19aAoafU"},"outputs":[],"source":["most_frequent_tokens = tokens_by_freq.sortBy(lambda x: -x[1]).map(lambda x: x[0]).take(30)\n","#puedo hacer un map para \"dar vuelta\" clave-valor y luego hacer sortByKey (en vez de sortBy)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DrhBKsUxq7I","outputId":"6254c68c-7f45-4e57-977b-8b93dfff7e3e","executionInfo":{"status":"ok","timestamp":1696865671129,"user_tz":180,"elapsed":25,"user":{"displayName":"FACUNDO NAHUEL FERNANDEZ","userId":"09349471394871366588"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.',\n"," 'i',\n"," ',',\n"," '!',\n"," 'it',\n"," 'game',\n"," 'the',\n"," \"'s\",\n"," 'good',\n"," 'like',\n"," 'app',\n"," 'this',\n"," 'great',\n"," 'love',\n"," 'get',\n"," 'time',\n"," '...',\n"," \"n't\",\n"," '?',\n"," 'would',\n"," 'really',\n"," 'even',\n"," 'not',\n"," 'ca',\n"," 'update',\n"," 'phone',\n"," 'you',\n"," 'work',\n"," \"'m\",\n"," 'please']"]},"metadata":{},"execution_count":9}],"source":["most_frequent_tokens"]},{"cell_type":"markdown","source":["# !!!!!!\n","Podría también haber paralelizado most_frequent_tokens y luego filtrar ese rdd por stopwords (no habría hecho falta cachear anteriormente), no sé verdaderamente qué es mejor.\n"],"metadata":{"id":"JGJPkJPCPTNi"}},{"cell_type":"markdown","metadata":{"id":"8LUxmlIruMXK"},"source":["Obtengo, inevitablemente, los 30 tokens más frecuentes en una lista. Puedo filtrar el rdd de tokens (el que tengo cacheado) con esta lista y la lista de stopwords.\n","\n","El map lo hago para mostrar únicamente los tokens, no es necesario si no importa ver la frecuencia de cada token en las reviews.\n","\n","Puedo hacer un collect ya que se que tengo un conjunto acotado de palabras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yugJKj5zkXNE"},"outputs":[],"source":["stopwords = set(stopwords.words('english'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TfA_yLBWAo1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"89db742d-5927-4490-8220-416861cd9d23","executionInfo":{"status":"ok","timestamp":1696865671607,"user_tz":180,"elapsed":483,"user":{"displayName":"FACUNDO NAHUEL FERNANDEZ","userId":"09349471394871366588"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'this', 'it', 'not', 'you', 'the']"]},"metadata":{},"execution_count":11}],"source":["tokens_by_freq.filter(lambda x: (x[0] in most_frequent_tokens) &  (x[0] in stopwords)).map(lambda x: x[0]).collect()"]},{"cell_type":"markdown","metadata":{"id":"ItDhqZ-upTbL"},"source":["Por \"culpa\" de las abreviaciones nos quedan separados \"n't\" de \"not\", nos queda \"'m\" en vez de \"am\", \"'s\" en vez de \"is\" y algo similar sucede con \"ca\", que probablemente queda así porque el tokenizer separa \"can't\" literalmente en dos, en vez de considerar que realmente es \"cannot\".\n","\n","Tanto am, is como can están entre las stopwords pero por aparecer abreviadas no se tienen en cuenta.\n","\n","Not debería tener mayor frecuencia juntado los not y los n't."]},{"cell_type":"markdown","metadata":{"id":"KvzVEM5gp4rD"},"source":["Podría haber eliminado símbolos (como el punto o la coma, que quedaron entre los primeros) de las reviews para obtener resultados solo con palabras, pero no sé si era la idea del ejercicio ya que estos pueden ser considerados como tokens."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}